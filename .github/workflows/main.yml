name: Run Ollama with Localtunnel on Linux

on:
  workflow_dispatch:

jobs:
  run-ollama:
    runs-on: ubuntu-latest
    steps:
      - name: Install Ollama on Linux
        shell: bash
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama server
        shell: bash
        run: |
          export OLLAMA_HOST=0.0.0.0:11434
          ollama serve &
          sleep 10

      - name: Verify Ollama is Running
        shell: bash
        run: |
          curl http://127.0.0.1:11434 || echo "Ollama not reachable locally"

      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'

      - name: Install Localtunnel
        shell: bash
        run: |
          npm install -g localtunnel
          curl https://loca.lt/mytunnelpassword

      - name: Start Localtunnel
        shell: bash
        run: |
          lt --port 11434 > tunnel_url.txt 2>&1 &
          sleep 5

      - name: Display and Test Localtunnel URL
        shell: bash
        run: |
          TUNNEL_URL=$(cat tunnel_url.txt | grep -o 'https://[^ ]*.loca.lt')
          echo "TUNNEL_URL=$TUNNEL_URL" >> $GITHUB_ENV
          echo "Ollama is accessible at $TUNNEL_URL"
          curl -H "x-requested-with: XMLHttpRequest" "$TUNNEL_URL" || echo "403 encountered; try bypassing warning page"

      - name: Keep workflow running for 5 hours
        shell: bash
        run: |
          sleep 18000
